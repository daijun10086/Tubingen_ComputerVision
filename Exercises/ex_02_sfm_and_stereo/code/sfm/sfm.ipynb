{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Lecture - Exercise 2 Part 1 - Structure-from-Motion\n",
    "\n",
    "This first part of exercise 2 contains hands-on coding problems in two-frame structure from motion. More specifically, given a pair of views with known camera intrinsics and matched correspondence points based on SIFT features, we will implement the 8-point algorithm to recover both the fundamental and essential matrix. We will look at both the \"vanilla\" and the normalized version and compare their robustness to noisy correspondence points. In a next step, we will implement triangulation via the DLT, which you are already familiar with from the previous exercise. With that\n",
    "we have all the necessary building blocks to recover both the relative pose of the two views and a 3d-reconstruction of the scene.\n",
    "\n",
    "As in the previous exercise, this notebook guides you through the relevant steps. When you see helper functions, you don't need to do anything - they are already implemented. The functions you need to implement are indicated as Exercise Function and where applicable the sections you need to fill in are marked. Sometimes, you can find Hints - these are written upside-down so you can first try to find the solution without reading them.\n",
    "\n",
    "Happy hacking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Let's first import relevant libaries, load the images/views along with their calibration data and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to install dependencies in google colab\n",
    "# !pip install opencv-python==4.5.1.48\n",
    "# !pip install ipympl\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.cvtColor(cv2.imread('./img1.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('./img2.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load cameras file\n",
    "camera_dict = np.load('./cameras.npz')\n",
    "# camera1\n",
    "scale_mat1 = camera_dict['scale_mat_%d' % 45].astype(np.float32)\n",
    "world_mat1 = camera_dict['world_mat_%d' % 45].astype(np.float32)\n",
    "proj_mat1 = (world_mat1 @ scale_mat1)[:3, :4]\n",
    "K1 = cv2.decomposeProjectionMatrix(proj_mat1)[0]\n",
    "K1 = K1/K1[2, 2]\n",
    "# camera2\n",
    "scale_mat2 = camera_dict['scale_mat_%d' % 41].astype(np.float32)\n",
    "world_mat2 = camera_dict['world_mat_%d' % 41].astype(np.float32)\n",
    "proj_mat2 = (world_mat2 @ scale_mat2)[:3, :4]\n",
    "K2 = cv2.decomposeProjectionMatrix(proj_mat2)[0]\n",
    "K2 = K2/K2[2, 2]\n",
    "\n",
    "# Let's visualize the images\n",
    "f = plt.figure(figsize=(15, 5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.imshow(img1)\n",
    "ax2.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find correspondence points for the two views. For this we provide a helper function which first finds salient points using SIFT and then performs k-nearest neighbour matching based on these SIFT features. To remove outliers we perform the ratio test. SIFT is just one of many descriptors implemented in OpenCV - feel free to try some others and also play around with the ratio parameter for outlier removal. Once computed, we can visualize the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def get_keypoints(img1, img2):\n",
    "    ''' \n",
    "    Finds correspondence points between two images by searching for salient \n",
    "    points via SIFT and then searching for matches via k-nearest neighours.\n",
    "    Performs a ratio test to filter for outliers.\n",
    "    \n",
    "    Args:\n",
    "        img1 (np.ndarray): image, first view\n",
    "        img2 (np.ndarray): image, second view\n",
    "    Returns:\n",
    "        p_source(np.ndarray): Nx3 array of correspondence points in first view\n",
    "            in homogenous image coordinates.\n",
    "        p_source(np.ndarray): Nx3 array of correspondence points in second view\n",
    "            in homogenous image coordinates.\n",
    "    '''\n",
    "    # Initialize feature description algorithm\n",
    "    # We are going to use SIFT but OpenCV provides \n",
    "    # implementations for many more - feel free to try them!\n",
    "    descriptor = cv2.SIFT_create(nfeatures=10000)\n",
    "\n",
    "    keypoints1, features1 = descriptor.detectAndCompute(img1, None)\n",
    "    keypoints2, features2 = descriptor.detectAndCompute(img2, None)\n",
    "    \n",
    "    # Initialize matching algorithm\n",
    "    # We are going to use k-nearest neighbours with L2 as the distance measure\n",
    "    bf = cv2.BFMatcher_create(cv2.NORM_L2)\n",
    "\n",
    "    # Find matching points\n",
    "    matches = bf.knnMatch(features1, features2, k=2)\n",
    "    \n",
    "    # Remove outliers via ratio test\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    # Select matched points\n",
    "    keypoints1 = np.float32(\n",
    "        [ keypoints1[good_match.queryIdx].pt for good_match in good ]\n",
    "    ).reshape(-1,2)\n",
    "    keypoints2 = np.float32(\n",
    "        [ keypoints2[good_match.trainIdx].pt for good_match in good ]\n",
    "    ).reshape(-1,2)\n",
    "\n",
    "    # Augment point vectors\n",
    "    N = keypoints1.shape[0]\n",
    "    keypoints1 = np.concatenate([keypoints1, np.ones((N, 1))], axis=-1)\n",
    "    keypoints2 = np.concatenate([keypoints2, np.ones((N, 1))], axis=-1)\n",
    "\n",
    "    return keypoints1, keypoints2\n",
    "\n",
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def draw_matches(img1, img2, keypoints1, keypoints2):\n",
    "    ''' \n",
    "    Returns a visualization of correspondences accross two images.\n",
    "\n",
    "    Args:\n",
    "        img1 (np.ndarray): image, first view\n",
    "        img2 (np.ndarray): image, second view\n",
    "        keypoints1 (np.ndarray): Nx3 array of correspondence points in first \n",
    "            view in homogenous image coordinates.\n",
    "        keypoints2 (np.ndarray): Nx3 array of correspondence points in second \n",
    "            view in homogenous image coordinates.\n",
    "\n",
    "    Returns:\n",
    "        output_img (np.ndarray): image, horizontally stacked first and sedond\n",
    "        view with correspondence points overlaid.    \n",
    "    '''\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # stack views horizontally\n",
    "    output_img = np.zeros((max([h1, h2]), w1 + w2, 3), dtype='uint8')\n",
    "    output_img[:h1, :w1, :] = np.dstack([img1])\n",
    "    output_img[:h2, w1:w1 + w2, :] = np.dstack([img2])\n",
    "\n",
    "    # draw correspondences\n",
    "    # we only visualize only a subset for clarity\n",
    "    for p1, p2 in zip(keypoints1[::4], keypoints2[::4]):\n",
    "        (x1, y1) = p1[:2]\n",
    "        (x2, y2) = p2[:2]\n",
    "\n",
    "        cv2.circle(output_img, (int(x1), int(y1)), 10, (0, 255, 255), 10)\n",
    "        cv2.circle(output_img, (int(x2) + w1, int(y2)), 10, (0, 255, 255), 10)\n",
    "\n",
    "        cv2.line(output_img, (int(x1), int(y1)), (int(x2) + w1, int(y2)), (0, 255, 255), 5)\n",
    "\n",
    "    return output_img\n",
    "\n",
    "# compute and visualize correspondences\n",
    "keypoints1, keypoints2 = get_keypoints(img1, img2)\n",
    "correspondence_vis = draw_matches(img1, img2, keypoints1, keypoints2)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.imshow(correspondence_vis)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The 8-Point Algorithm\n",
    "\n",
    "Now that we have sensible correspondence points we can begin recovering some geometry.\n",
    "\n",
    "### a)\n",
    "\n",
    "In the skeleton below, implement the standard version of the 8-point algorithm (no normalization) to compute the fundamental matrix from a set of corresponding points in homogenous image coordinates.\n",
    "\n",
    "*Hint 1*: ˙suoᴉʇɔǝɹᴉp ʎɐɹ lɐɔol ɟo pɐǝʇsuᴉ sǝʇɐuᴉpɹooɔ ǝƃɐɯᴉ ɯoɹɟ punoɟ ʇsnɾ 'ǝɹnʇɔǝl ǝɥʇ uᴉ uʍoɥs sɐ xᴉɹʇɐɯ lɐᴉʇuǝssǝ ǝɥʇ ɹoɟ sɐ ǝɯɐs ǝɥʇ sʞool ɯǝʇsʎs ɹɐǝuᴉl ǝɥ┴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def compute_fundamental_matrix(keypoints1, keypoints2):\n",
    "    ''' \n",
    "    Computes the fundamental matrix from image coordinates using the 8-point \n",
    "    algorithm by constructing and solving the corresponding linear system.\n",
    "\n",
    "    Args:\n",
    "        keypoints1 (np.ndarray): Nx3 array of correspondence points in first \n",
    "            view in homogenous image coordinates.\n",
    "        keypoints2 (np.ndarray): Nx3 array of correspondence points in second \n",
    "            view in homogenous image coordinates.\n",
    "\n",
    "    Returns:\n",
    "        F (np.ndarray): 3x3 fundamental matrix.\n",
    "    '''\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 1)\n",
    "    # -------------------------------------\n",
    "\n",
    "    return F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our result by visualizing the epipolar lines. How can we tell if the result is sensible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fundamental matrix using 8-point algorithm\n",
    "F = compute_fundamental_matrix(keypoints1, keypoints2)\n",
    "\n",
    "# Compute epipolar lines\n",
    "el2 = np.swapaxes(F @ keypoints1.swapaxes(0, 1), 0, 1)\n",
    "el1 = np.swapaxes(F.transpose() @ keypoints2.swapaxes(0, 1), 0, 1)\n",
    "\n",
    "\n",
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def draw_epipolar_lines(img1, img2, els1, els2, kps1, kps2):\n",
    "    ''' \n",
    "    Returns an image with epipolar lines drawn onto the images.\n",
    "\n",
    "    Args:\n",
    "        img1 (np.ndarray): image, first view\n",
    "        img2 (np.ndarray): image, second view\n",
    "        els1 (np.ndarray): Nx3 array of epipolar lines in the first view \n",
    "            induced by correspondences in the second view.\n",
    "        els2 (np.ndarray): Nx3 array of epipolar lines in the second view \n",
    "            induced by correspondences in the first view.    \n",
    "        keypoints1 (np.ndarray): Nx3 array of correspondence points in first \n",
    "            view in homogenous image coordinates.\n",
    "        keypoints2 (np.ndarray): Nx3 array of correspondence points in second \n",
    "            view in homogenous image coordinates.\n",
    "\n",
    "    Returns:\n",
    "        output_img (np.ndarray): image, horizontally stacked first and sedond\n",
    "        view with correspondence points overlaid.\n",
    "    '''\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    output_img = np.zeros((max([h1, h2]), w1 + w2, 3), dtype='uint8')\n",
    "    output_img[:h1, :w1, :] = np.dstack([img1])\n",
    "    output_img[:h2, w1:w1 + w2, :] = np.dstack([img2])\n",
    "\n",
    "    for p1, el1, p2, el2 in zip(kps1, els1, kps2, els2):\n",
    "        (x1, y1) = p1[:2]\n",
    "        (x2, y2) = p2[:2]\n",
    "\n",
    "        el1_x_0, el1_y_0 = map(int, [0, -el1[2]/el1[1]])\n",
    "        el1_x_1, el1_y_1 = map(\n",
    "            int, [w1, (-el1[0] / el1[1]) * w1 - el1[2]/el1[1]]\n",
    "        )\n",
    "        el2_x_0, el2_y_0 = map(int, [0, -el2[2]/el2[1]])\n",
    "        el2_x_1, el2_y_1 = map(\n",
    "            int, [w2, (-el2[0] / el2[1]) * w2 - el2[2]/el2[1]]\n",
    "        )\n",
    "\n",
    "        cv2.circle(output_img, (int(x1), int(y1)), 10, (255, 0, 255), 10)\n",
    "        cv2.circle(output_img, (int(x2) + w1, int(y2)), 10, (0, 255, 255), 10)\n",
    "\n",
    "        cv2.line(output_img, (el1_x_0, el1_y_0), (el1_x_1, el1_y_1), (0, 255, 255), 5)\n",
    "        cv2.line(output_img, (el2_x_0 + w1, el2_y_0), (el2_x_1 + w1, el2_y_1), (255, 0, 255), 5)\n",
    "\n",
    "    return output_img\n",
    "\n",
    "# Plot epipolar lines\n",
    "el_vis = draw_epipolar_lines(\n",
    "    img1, img2, el1[0:5], el2[0:5], keypoints1[0:5], keypoints2[0:5]\n",
    ")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.imshow(el_vis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have fairly accurate correspondences, so the standard 8-point algorithm already works. But, as you know from lecture 3, the multiplicative measurement terms in our system of equations make the standard version of the algorihtm sensitive to measurement noise. Let's add some artificial noise and see how that affects the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise to correspondences\n",
    "keypoints_noisy1 = np.copy(keypoints1)\n",
    "keypoints_noisy2 = np.copy(keypoints2)\n",
    "keypoints_noisy1[..., :2] += np.random.normal(0, 2, size=keypoints_noisy1[..., :2].shape)\n",
    "keypoints_noisy2[..., :2] += np.random.normal(0, 2, size=keypoints_noisy2[..., :2].shape)\n",
    "\n",
    "F = compute_fundamental_matrix(\n",
    "    keypoints_noisy1,\n",
    "    keypoints_noisy2\n",
    ")\n",
    "\n",
    "# Compute epipolar lines\n",
    "el2 = np.swapaxes(F @ keypoints1.swapaxes(0, 1), 0, 1)\n",
    "el1 = np.swapaxes(F.transpose() @ keypoints2.swapaxes(0, 1), 0, 1)\n",
    "\n",
    "# Plot epipolar lines\n",
    "el_vis = draw_epipolar_lines(\n",
    "    img1, img2, el1[0:5], el2[0:5], keypoints1[0:5], keypoints2[0:5]\n",
    ")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.imshow(el_vis)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Luckily, we can make this algorithm more robust with just a few lines of code: 1.) We first normalize the correspondence points to zero-mean and unit variance, 2.) run the 8-point algorithm on these normalized points and 3.) finally back-transform the resulting fundamental matrix. This is the normalized 8-point algorithm described in \"In Defense of the 8-Point Algorithm\" (Hartley, TPAMI, 1997).\n",
    "\n",
    "### b)\n",
    "Implement this algorithm in the following function skeleton. You can reuse exercise function 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def compute_fundamental_matrix_normalized(keypoints1, keypoints2):\n",
    "    ''' \n",
    "    Computes the fundamental matrix from image coordinates using the normalized\n",
    "    8-point algorithm by first normalizing the keypoint coordinates to zero-mean \n",
    "    and unit variance, then constructing and solving the corresponding linear \n",
    "    system and finally undoing the normaliziation by back-transforming the \n",
    "    resulting matrix.\n",
    "\n",
    "    Args:\n",
    "        keypoints1 (np.ndarray): Nx3 array of correspondence points in first \n",
    "            view in homogenous image coordinates.\n",
    "        keypoints2 (np.ndarray): Nx3 array of correspondence points in second \n",
    "            view in homogenous image coordinates.\n",
    "\n",
    "    Returns:\n",
    "        F (np.ndarray): 3x3 fundamental matrix.\n",
    "    '''\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 2)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare both variants of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F from noisy correspondences using normalized 8-point algorithm\n",
    "F_normalized = compute_fundamental_matrix_normalized(\n",
    "    keypoints_noisy1,\n",
    "    keypoints_noisy2\n",
    ")\n",
    "\n",
    "el2_normalized = np.swapaxes(F_normalized @ keypoints1.swapaxes(0, 1), 0, 1)\n",
    "el1_normalized = np.swapaxes(F_normalized.transpose() @ keypoints2.swapaxes(0, 1), 0, 1)\n",
    "\n",
    "el_vis_normalized = draw_epipolar_lines(\n",
    "    img1, img2, \n",
    "    el1_normalized[0:5], el2_normalized[0:5], \n",
    "    keypoints1[0:5], keypoints2[0:5]\n",
    ")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax1.imshow(el_vis)\n",
    "ax2.imshow(el_vis_normalized)\n",
    "plt.show()\n",
    "\n",
    "# For the following sections we recompute F without artificial noise\n",
    "F_normalized = compute_fundamental_matrix_normalized(keypoints1, keypoints2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Given this robust estimation of the fundamental matrix and the intrinsics for both cameras/views we can compute the essential matrix. Implement this in the skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def compute_essential_matrix(F, K1, K2):\n",
    "    ''' \n",
    "    Computes the essential from the fundamental matrix given known intrinsics.\n",
    "\n",
    "    Args:\n",
    "        F (np.ndarray): 3x3 fundamental matrix.\n",
    "        K1 (np.ndarray): The 3x3 calibration matrix K for the first \n",
    "            view/camera.\n",
    "        K2 (np.ndarray): The 3x3 calibration matrix K for the second\n",
    "            view/camera.\n",
    "\n",
    "    Returns:\n",
    "        E (np.ndarray): 3x3 essential matrix.\n",
    "    '''\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 3)\n",
    "    # -------------------------------------\n",
    "    return E"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangulation\n",
    "\n",
    "Next we decompose the essential matrix to recover the relative pose of the second camera/view with respect to the first one. With this we can reconstruct the scene by triangulating correspondences. Unfortunately, this decomposition yields four possible solutions (See Szeliski book p. 685-686 for details). But, we can find the correct solution by doing triangulation with all of them and selecting the one that results in the highest number of triangulated points that lie in front of both cameras. This is called the chirality check.\n",
    "\n",
    "### d)\n",
    "In the skeleton below, implement a function that triangulates a point 'x_w' from its correspondeces in homogenous image coordinates 'x_1^s' and 'x_2^s' using the direct linear transform. You can use the provided helper functions to construct the projection matrix 'P' from intrinsics 'K', Rotation 'R' and translation 't' (see lecture 2) for convenience.\n",
    "\n",
    "*Hint 1*: ˙ɹoʇɔǝʌ uoᴉʇɐlsuɐɹʇ oɹǝz puɐ xᴉɹʇɐɯ uoᴉʇɐʇoɹ ʎʇᴉʇuǝpᴉ ǝɥʇ oʇ ʇǝs ǝq uɐɔ ɐɹǝɯɐɔ ʇsɹᴉɟ ǝɥʇ ɹoɟ sɔᴉsuᴉɹʇxǝ ǝɥ┴\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the essential matrix we can recover the relative rotation and\n",
    "# translation between views\n",
    "E = compute_essential_matrix(F_normalized, K1, K2)\n",
    "R1, R2, t = cv2.decomposeEssentialMat(E)\n",
    "\n",
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def assemble_pose_matrix(R, t):\n",
    "    ''' \n",
    "    Builds 4x4 pose matrix (extrinsics) from 3x3 rotation matrix and\n",
    "    3-d translation vector. See also lecture two.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): 3x3 rotation matrix.\n",
    "        t (np.ndarray): 3-d translation vector.\n",
    "\n",
    "    Returns:\n",
    "        pose (np.ndarray): 4x4 pose matrix (extrinsics).\n",
    "    '''\n",
    "     # augment R\n",
    "    R = np.concatenate([R, np.zeros([1, 3])], axis=0)\n",
    "    \n",
    "    # augment T\n",
    "    t = np.concatenate([t, np.ones([1, 1])])\n",
    "\n",
    "    # assemble and return pose matrix\n",
    "    return np.concatenate([R, t], axis=1)\n",
    "\n",
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def assemble_projection_matrix(K, R, t):\n",
    "    ''' \n",
    "    Builds 3x4 projection matrix from 3x3, calibration matrix, 3x3 rotation \n",
    "    matrix and 3-d translation vector. See also lecture two.\n",
    "\n",
    "    Args:\n",
    "        K (np.ndarray): 3x3 calibration matrix.\n",
    "        R (np.ndarray): 3x3 rotation matrix.\n",
    "        t (np.ndarray): 3-d translation vector.\n",
    "\n",
    "    Returns:\n",
    "        P (np.ndarray): 4x4 pose matrix.\n",
    "    '''\n",
    "    # TODO: use assemble pose\n",
    "    # augment K\n",
    "    K = np.concatenate([K, np.zeros([3, 1])], axis=1)\n",
    "\n",
    "    # augment R\n",
    "    R = np.concatenate([R, np.zeros([1, 3])], axis=0)\n",
    "    \n",
    "    # augment T\n",
    "    t = np.concatenate([t, np.ones([1, 1])])\n",
    "\n",
    "    # assemble and return camera matrix P\n",
    "    return K @ np.concatenate([R, t], axis=1)\n",
    "\n",
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def triangulate_point(keypoint1, keypoint2, K1, K2, R, t):\n",
    "    ''' \n",
    "    Triangulates world coordinates given correspondences from two views with\n",
    "    relative extrinsics R and t.\n",
    "\n",
    "    Args:\n",
    "        keypoints1 (np.ndarray): Nx3 array of correspondence points in first \n",
    "            view in homogenous image coordinates.\n",
    "        keypoints2 (np.ndarray): Nx3 array of correspondence points in second \n",
    "            view in homogenous image coordinates.\n",
    "        K1 (np.ndarray): The 3x3 calibration matrix K for the first \n",
    "            view/camera.\n",
    "        K2 (np.ndarray): The 3x3 calibration matrix K for the second\n",
    "            view/camera.\n",
    "        R (np.ndarray): 3x3 rotation matrix from first to second view.\n",
    "        t (np.ndarray): 3-d translation vector from first to second view.\n",
    "\n",
    "    Returns:\n",
    "        x_w (np.ndarray): Nx4 array of 3-d points in homogenous world \n",
    "            coordinates.\n",
    "    '''\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 4)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return x_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to do the chirality check and visualize the reconstructed scene as well as the camera/view poses! The resulting figure is interactive, so you can drag it with your mouse cursor to get different vantage points (unfortunately this does *not* work in google colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def chirality_check(keypoints1, keypoint2, K1, K2, R1, R2, t):\n",
    "    ''' \n",
    "    Triangulates world coordinates given correspondences from two views with\n",
    "    relative extrinsics R and T.\n",
    "\n",
    "    Args:\n",
    "        keypoints1 (np.ndarray): Nx3 array of correspondence points in first \n",
    "            view in homogenous image coordinates.\n",
    "        keypoints2 (np.ndarray): Nx3 array of correspondence points in second \n",
    "            view in homogenous image coordinates.\n",
    "        K1 (np.ndarray): The 3x3 calibration matrix K for the first \n",
    "            view/camera.\n",
    "        K2 (np.ndarray): The 3x3 calibration matrix K for the second\n",
    "            view/camera.\n",
    "        R1 (np.ndarray): first possible 3x3 rotation matrix from first to \n",
    "            second view.\n",
    "        R2 (np.ndarray): second possible 3x3 rotation matrix from first to \n",
    "            second view.\n",
    "        T (np.ndarray): 3-d translation vector from first to second view.\n",
    "\n",
    "    Returns:\n",
    "        pose (tuple of np.ndarrays): 3x3 rotation matrix and 3-d translation\n",
    "            vector from first to second view.\n",
    "        x_w (np.ndarray): Nx4 array of 3-d points in homogenous world \n",
    "            coordinates.\n",
    "    '''\n",
    "    solutions = [(R1, t), (R1, -t), (R2, t), (R2, -t)]\n",
    "    valid = [0, 0, 0, 0]\n",
    "    all_triangulations = []\n",
    "    for i, solution in enumerate(solutions):\n",
    "        triangulations = []\n",
    "        for kp1, kp2 in zip(keypoints1, keypoints2):\n",
    "            # triangulate points with respect to reference camera\n",
    "            x_w = triangulate_point(\n",
    "                kp1, kp2, \n",
    "                K1, K2,\n",
    "                solution[0], solution[1],\n",
    "            )\n",
    "            triangulations.append(x_w)\n",
    "\n",
    "            # perform chirality check\n",
    "            visible = False\n",
    "            # visibility in reference view\n",
    "            if x_w[2] > 0 and x_w[2] < 50:\n",
    "                visible = True\n",
    "                \n",
    "            # visibility in other view \n",
    "            x_w = assemble_projection_matrix(K1, solution[0], solution[1]) @ x_w\n",
    "\n",
    "            if x_w[2] > 0 and x_w[2] < 50:\n",
    "                visible = True\n",
    "            else:\n",
    "                visible = False\n",
    "\n",
    "            # increment number of valid points if visible in both\n",
    "            if visible:\n",
    "                valid[i] += 1\n",
    "\n",
    "        # collect triangulations for all solutions \n",
    "        # so we don't have to recompute them\n",
    "        all_triangulations.append(\n",
    "            np.array(triangulations, np.float32)\n",
    "        )\n",
    "\n",
    "    # return the solution for which the most points are visible in both cameras\n",
    "    return solutions[np.argmax(valid)], all_triangulations[np.argmax(valid)]\n",
    "\n",
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def draw_camera(ax, pose, K):\n",
    "    ''' \n",
    "    Draws a camera coordinate frame in 3d plot. \n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib axes): The figure in which to draw the camera.\n",
    "        pose (np.ndarray): 4x4 pose matrix (extrinsics).\n",
    "        K (np.ndarray): 3x3 calibration matrix (intrinsics).\n",
    "\n",
    "    Returns:\n",
    "        ax (matplotlib): Figure with camera added.\n",
    "    '''\n",
    "    # set up unit vectors\n",
    "    scale = K[0, 0] / 10000\n",
    "    o = np.array([0, 0, 0, 1])\n",
    "    x = np.array([1 * scale , 0, 0, 1])\n",
    "    y = np.array([0, 1 * scale, 0, 1])\n",
    "    z = np.array([0, 0, 1 * scale, 1])\n",
    "\n",
    "    pose = np.linalg.inv(pose)\n",
    "    o_prime = pose @ o\n",
    "    x_prime = pose @ x\n",
    "    y_prime = pose @ y\n",
    "    z_prime = pose @ z\n",
    "\n",
    "    ax.plot(\n",
    "        [o_prime[0], x_prime[0]], [o_prime[2], x_prime[2]],  [-o_prime[1], -x_prime[1]], c='r'\n",
    "    )\n",
    "    ax.plot(\n",
    "        [o_prime[0], y_prime[0]], [o_prime[2], y_prime[2]], [-o_prime[1], -y_prime[1]], c='b'\n",
    "    )\n",
    "    ax.plot(\n",
    "        [o_prime[0], z_prime[0]], [o_prime[2], z_prime[2]], [-o_prime[1], -z_prime[1]], c='g'\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "pose2, triangulations = chirality_check(keypoints1, keypoints2, K1, K2, R1, R2, t)\n",
    "\n",
    "# Draw results\n",
    "%matplotlib widget\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.imshow(draw_matches(img1, img2, [], []))\n",
    "ax2 = fig.add_subplot(212, projection='3d')\n",
    "ax2.scatter(\n",
    "    triangulations[:, 0], \n",
    "    triangulations[:, 2], \n",
    "    - triangulations[:, 1], \n",
    "    c='r', marker='o',\n",
    ")\n",
    "\n",
    "P1 = assemble_pose_matrix(np.eye(3), np.zeros([3, 1]))\n",
    "ax2 = draw_camera(ax2, P1, K1)\n",
    "P2 = assemble_pose_matrix(pose2[0], pose2[1])\n",
    "ax2 = draw_camera(ax2, P2, K2)\n",
    "ax2.set_xlabel('X Axis')\n",
    "ax2.set_ylabel('Z Axis')\n",
    "ax2.set_zlabel('Y Axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have reconstructed our first scene! Let's move on to the next part of this exercise: Stereo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "674dd5b1a05b52894ba0247ee6819334144aec94e75909d6be66ea1305985604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
