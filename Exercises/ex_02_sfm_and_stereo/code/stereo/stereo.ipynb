{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Lecture - Exercise 2 Part 2 - Stereo\n",
    "\n",
    "This second part of exercise 2 deals with stereo vision and disparity estimation in particular. Here, you are first going to implement the block matching algorithm as a classical approach using handcrafted similarity measures and visualize the results. Next, we will move on to learned stereo matching using Siamese Neural Networks.\n",
    "\n",
    "As in the previous exercise, this notebook guides you through the relevant steps. When you see helper functions, you don't need to do anything - they are already implemented. The functions you need to implement are indicated as Exercise Function and where applicable the sections you need to fill in are marked. Sometimes, you can find Hints - these are written upside-down so you can first try to find the solution without reading them.\n",
    "\n",
    "Happy hacking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Let's first the import relevant libaries, set some hyperparameters like `window_size` and `max_disparity` and load a test set of five stereo image pairs from the KITTI dataset. We'll visualize one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment in google colab to extract dataset\n",
    "# !unzip KITTI_2015_subset.zip\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from stereo_batch_provider import KITTIDataset, PatchProvider\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# Shortcuts\n",
    "input_dir = './KITTI_2015_subset'\n",
    "window_size = 3\n",
    "max_disparity = 50\n",
    "out_dir = os.path.join(\n",
    "    './output/handcrafted_stereo', 'window_size_%d' % window_size\n",
    ")\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "# Load dataset\n",
    "dset = KITTIDataset(os.path.join(input_dir, \"data_scene_flow/testing/\"))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.imshow(dset[0][0].squeeze(), cmap='gray')\n",
    "ax1.title.set_text('Left Image')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(dset[0][1].squeeze(), cmap='gray')\n",
    "ax2.title.set_text('Right Image')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Matching\n",
    "\n",
    "In this section we will implement the block matching algorithm for disparity estimation. Similar to the *Sum of Squared Differences (SSD)* metric, which you know from the lecture, we will use the *Sum of Absolute Differences (SAD)*, which is defined as:\n",
    "\n",
    "$SAD(x, y, d) = |w_{L}(x, y) - w_{R}(x - d, y)|$\n",
    "\n",
    "where $d$ is in the range of $[0, D]$ and the maximum disparity $D$ is a hyperparameter.\n",
    "\n",
    "### a)\n",
    "In the function skeleton below, implement block matching using *SAD* to compute a disparity map given a stereo image pair, a window size and a maximum disparity. Use the *Winner-Takes-All (WTA)* approach to determine the final disparity for each pixel.\n",
    "\n",
    "*Hint*: ˙ɥʇpᴉʍ puɐ ʇɥƃᴉǝɥ s’ǝƃɐɯᴉ ǝɥʇ ǝɹɐ ɥʇpᴉʍ puɐ ʇɥƃᴉǝɥ ǝɹǝH ˙lǝxᴉd ɥɔɐǝ ɹoɟ ʎʇᴉɹɐdsᴉp ǝɥʇ ǝʇɐlnɔlɐɔ\n",
    "oʇ [Ɩ + ǝzᴉs ʍopuᴉʍ − ɥʇpᴉʍ '0] puɐ [Ɩ + ǝzᴉs ʍopuᴉʍ − ʇɥƃᴉǝɥ '0] ɯoɹɟ ƃuᴉƃuɐɹ sdool ɹoɟ pǝʇsǝu oʍʇ\n",
    "ǝsn uɐɔ noʎ ʇɐɥʇ os sǝƃɐɯᴉ ǝɥʇ oʇ ƃuᴉppɐd oɹǝz ʎlddɐ ǝʍ ʇɐɥʇ ǝǝs uɐɔ noʎ ǝpoɔ ǝɥʇ uᴉ\n",
    "\n",
    "*Bonus*: Instead, you can also try to implement a more elegant solution where for each candidate disparity value you first compute the absolute difference image and next convolve this image with a mean filter of size `window_size`, using the `scipy.signal.convolve()` function. We provide a bonus function skeleton for this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def add_padding(I, padding):\n",
    "    \"\"\"\n",
    "    Adds zero padding to an RGB or grayscale image.\n",
    "\n",
    "    Args:\n",
    "        I (np.ndarray): HxWx? numpy array containing RGB or grayscale image\n",
    "    \n",
    "    Returns:\n",
    "        P (np.ndarray): (H+2*padding)x(W+2*padding)x? numpy array containing zero padded image\n",
    "    \"\"\"\n",
    "    if len(I.shape) == 2:\n",
    "        H, W = I.shape\n",
    "        padded = np.zeros((H+2*padding, W+2*padding), dtype=np.float32)\n",
    "        padded[padding:-padding, padding:-padding] = I\n",
    "    else:\n",
    "        H, W, C = I.shape\n",
    "        padded = np.zeros((H+2*padding, W+2*padding, C), dtype=I.dtype)\n",
    "        padded[padding:-padding, padding:-padding] = I\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def sad(image_left, image_right, window_size=3, max_disparity=50):\n",
    "    \"\"\"\n",
    "    Compute the sum of absolute differences between image_left and image_right.\n",
    "\n",
    "    Args:\n",
    "        image_left (np.ndarray): HxW numpy array containing grayscale right image\n",
    "        image_right (np.ndarray): HxW numpy array containing grayscale left image\n",
    "        window_size: window size (default 3)\n",
    "        max_disparity: maximal disparity to reduce search range (default 50)\n",
    "    \n",
    "    Returns:\n",
    "        D (np.ndarray): HxW numpy array containing the disparity for each pixel\n",
    "    \"\"\"\n",
    "\n",
    "    D = np.zeros_like(image_left)\n",
    "\n",
    "    # add zero padding\n",
    "    padding = window_size // 2\n",
    "    image_left = add_padding(image_left, padding).astype(np.float32)\n",
    "    image_right = add_padding(image_right, padding).astype(np.float32)\n",
    "    \n",
    "    height = image_left.shape[0]\n",
    "    width = image_left.shape[1]\n",
    "\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 1)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return D\n",
    "\n",
    "\n",
    "###########################\n",
    "##### Bonus Function #####\n",
    "###########################\n",
    "def sad_convolve(image_left, image_right, window_size=3, max_disparity=50):\n",
    "    \"\"\"\n",
    "    Compute the sum of absolute differences between image_left and image_right\n",
    "    by using a mean filter.\n",
    "\n",
    "    Args:\n",
    "        image_left (np.nfarray): HxW numpy array containing grayscale right image\n",
    "        image_right (np.nfarray): HxW numpy array containing grayscale left image\n",
    "        window_size: window size (default 3)\n",
    "        max_disparity: maximal disparity to reduce search range (default 50)\n",
    "    \n",
    "    Returns:\n",
    "        D (np.ndarray): HxW numpy array containing the disparity for each pixel\n",
    "    \"\"\"\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 1 BONUS)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return D\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "To check if our computed disparity is sensible it's always a good idea to visualize the results. Implement a function in the skeleton below that plots both the estimated disparity map as well as one of the input images.\n",
    "\n",
    "Remark: *In case you end up using matplotlib for your visualizations, keep in mind that you might have to\n",
    "properly adjust the figure size and the v min and v max arguments of the imshow() function in order\n",
    "to get nice results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def visualize_disparity(disparity, im_left, im_right, title='Disparity Map', max_disparity=50):\n",
    "    \"\"\"\n",
    "    Generates a visualization for the disparity map.\n",
    "\n",
    "    Args:\n",
    "        disparity (np.array): disparity map\n",
    "        title: plot title\n",
    "        out_file: output file path\n",
    "        max_disparity: maximum disparity\n",
    "    \"\"\"\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 2)\n",
    "    # -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's compute the disparities for our dataset and see how they look! This might take up to several minutes per scene depending on your hardware. If you implemented the bonus function you can replace 'sad()' with 'sad_convolve()' below, which should bring a significant speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dset)):\n",
    "        # Load left and right images\n",
    "        im_left, im_right  = dset[i]\n",
    "        im_left, im_right = im_left.squeeze(-1), im_right.squeeze(-1)\n",
    "\n",
    "        # Calculate disparity\n",
    "        D = sad(im_left, im_right, window_size=window_size, max_disparity=max_disparity)\n",
    "\n",
    "        # Define title for the plot\n",
    "        title = 'Disparity map for image %04d with block matching (window size %d)' % (i, window_size)\n",
    "        # Define output file name and patch\n",
    "        file_name = '%04d_w%03d.png' % (i, window_size)\n",
    "        out_file_path = os.path.join(out_dir, file_name)\n",
    "\n",
    "        # Visualize the disparty and save it to a file\n",
    "        visualize_disparity(D, im_left, im_right, title=title, max_disparity=max_disparity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Experiment with the different window sizes {3, 7, 15} and report which one leads to better visual results and why? (You can open the plots in new tabs to retain old results when you re-run the cell. In case you were not able to solve the previous exercises, you can use the provided disparity maps in the examples/ folder.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Why do you think the block matching approach fails to lead to good estimations around homogeneous regions such as the road?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Neural Networks\n",
    "\n",
    "In this section we again estimate the disparity for all the stereo pairs in the test set but this time\n",
    "using a Siamese Neural Network which will learn the task from data. The key idea is to train the network\n",
    "to estimate the similarity between two patches. We do this by first extracting features in a convolutional\n",
    "manner and then normalizing the per-pixel feature vectors and calculating the dot product.\n",
    "\n",
    "### e)\n",
    "\n",
    "Let's start by building our Siamese Neural Network. Using the `torch.nn` library, fill in the missing code in the PyTorch Module skeleton `class StereoMatchingNetwork(torch.nn.module)` provided below for both the `def __init__(self)` and `def forward(self, X)` methods to define the architecture. It should consist of the following:\n",
    "\n",
    "- `Conv2d(..., out_channels=64, kernel_size=3)`\n",
    "- `ReLU()`\n",
    "- `Conv2d(..., out_channels=64, kernel_size=3)`\n",
    "- `ReLU()`\n",
    "- `Conv2d(..., out_channels=64, kernel_size=3)`\n",
    "- `ReLU()`\n",
    "- `Conv2d(..., out_channels=64, kernel_size=3)`\n",
    "- `functional.normalize(..., dim=1, p=2)`\n",
    "\n",
    "The `def forward(self, X)` method should then perform one forward pass through those layers. Be aware that the layers form only one of the two identical branches of the siamese network. The forward pass will be called twice, that ensures weight sharing between both branches of the siamese network.\n",
    "\n",
    "Remark: *Note that the convolutional layers expect the data to have shape batch size × channels ×\n",
    "height × width. Permute the input dimensions accordingly for the convolutions and remember to\n",
    "revert it before returning the features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "class StereoMatchingNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Implementation of the network layers.\n",
    "        Layer output tensor size: (batch_size, n_features, height - 8, width - 8)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gpu = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        #######################################\n",
    "        # -------------------------------------\n",
    "        # TODO: ENTER CODE HERE (EXERCISE 5)\n",
    "        # -------------------------------------\n",
    "\n",
    "        # Hint: Don't forget to move the modules to the gpu\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" \n",
    "        The forward pass of the network. Returns the features for a given image patch.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): image patch of shape (batch_size, height, width, n_channels)\n",
    "\n",
    "        Returns:\n",
    "            features (torch.Tensor): predicted normalized features of the input image patch X,\n",
    "                               shape (batch_size, height - 8, width - 8, n_features)\n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        # -------------------------------------\n",
    "        # TODO: ENTER CODE HERE (EXERCISE 5)\n",
    "        # -------------------------------------\n",
    "\n",
    "        return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "From the lecture you know two classes of Siamese Neural Network architectures - which class does this architecture belong to?\n",
    "\n",
    "### g)\n",
    "\n",
    "Next, we need a way to compute a similarity score from the features extracted by the Siamese Neural Network. In the skeleton below, implement a function which takes an instance of the `StereoMatchingNetwork(torch.nn.Module)` class, as well as a pair of patches from the left and right image as input and estimates their similarity.\n",
    "\n",
    "*Hint 1*: ˙ sɹosuǝʇ ǝɹnʇɐǝɟ ǝsǝɥʇ uǝǝʍʇǝq ʎʇᴉɹɐlᴉɯᴉs ǝɥʇ ǝʇɐlnɔlɐɔ uǝɥ┴ ˙ʞɹoʍʇǝu ǝsǝɯɐᴉS ǝɥʇ ɟo ɥɔuɐɹq ɥɔɐǝ ɟo sɹosuǝʇ ǝɹnʇɐǝɟ ǝɥʇ ǝʇɐɯᴉʇsǝ oʇ sɹosuǝʇ oʍʇ ǝɥʇ ɟo ɥɔɐǝ uo ssɐd pɹɐʍɹoɟ ǝɥʇ unɹ 'ʇsɹᴉℲ\n",
    "\n",
    "*Hint 2*: ˙ʇɔnpoɹd ʇop ǝɥʇ ƃuᴉʇɐlnɔlɐɔ ʎq ʎʇᴉɹɐlᴉɯᴉs ǝɥʇ uᴉɐʇqo noʎ 'pǝzᴉlɐɯɹou ʎpɐǝɹlɐ ǝɹɐ sɹosuǝʇ ǝɹnʇɐǝɟ ǝɥʇ s∀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def calculate_similarity_score(compute_features, Xl, Xr):\n",
    "    \"\"\"\n",
    "    Computes the similarity score for two stereo image patches.\n",
    "\n",
    "    Args:\n",
    "        compute_features (torch.nn.Module):  pytorch module object\n",
    "        Xl (torch.Tensor): tensor holding the left image patch\n",
    "        Xr (torch.Tensor): tensor holding the right image patch\n",
    "\n",
    "    Returns:\n",
    "        score (torch.Tensor): the similarity score of both image patches which is the dot product of their features\n",
    "    \"\"\"\n",
    "    #######################################\n",
    "    # -------------------------------------\n",
    "    # TODO: ENTER CODE HERE (EXERCISE 7)\n",
    "    # -------------------------------------\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As of now, the network's weights are randomly initialized. In order for it to compute useful features we need to train it for the task. For this we need a loss function to quantify how well the network performs at the task and a training loop to run the optimization to minimize that loss function. Both are provided as helper functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def hinge_loss(score_pos, score_neg, label):\n",
    "    \"\"\"\n",
    "    Computes the hinge loss for the similarity of a positive and a negative example.\n",
    "\n",
    "    Args:\n",
    "        score_pos (torch.Tensor): similarity score of the positive example\n",
    "        score_neg (torch.Tensor): similarity score of the negative example\n",
    "        label (torch.Tensor): the true labels\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (torch.Tensor): the mean loss over the patch and the mini batch\n",
    "        acc (torch.Tensor): the accuracy of the prediction\n",
    "    \"\"\"\n",
    "    # Calculate the hinge loss max(0, margin + s_neg - s_pos)\n",
    "    loss = torch.max(0.2 + score_neg - score_pos, torch.zeros_like(score_pos))\n",
    "\n",
    "    # Obtain the mean over the patch and the mini batch\n",
    "    avg_loss = torch.mean(loss)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    similarity = torch.stack([score_pos, score_neg], dim=1)\n",
    "    labels = torch.argmax(label, dim=1)\n",
    "    predictions = torch.argmax(similarity, dim=1)\n",
    "    acc = torch.mean((labels == predictions).float())\n",
    "\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def training_loop(infer_compute_features, patches, optimizer, iterations=1000, batch_size=128):\n",
    "    '''\n",
    "    Runs the training loop of the siamese network.\n",
    "    \n",
    "    Args:\n",
    "        compute_features (obj): pytorch module\n",
    "        patches (obj): patch provider object\n",
    "        optimizer (obj): optimizer object\n",
    "        iterations (int): number of iterations to perform\n",
    "        batch_size (int): batch size\n",
    "    '''\n",
    "\n",
    "    loss_list = []\n",
    "    try:\n",
    "        print(\"Starting training loop.\")\n",
    "        for idx, batch in zip(range(iterations), patches.iterate_batches(batch_size)):\n",
    "            # Extract the batches and labels\n",
    "            Xl, Xr_pos, Xr_neg = batch\n",
    "            # uncomment if you don't have a gpu\n",
    "            # Xl, Xr_pos, Xr_neg = Xl.cpu(), Xr_pos.cpu(), Xr_neg.cpu()\n",
    "            \n",
    "            # use this line if you have a gpu\n",
    "            label = torch.eye(2).cuda()[[0]*len(Xl)]  # label is always [1, 0]\n",
    "            # use this line if you don't have a gpu\n",
    "            # label = torch.eye(2)[[0]*len(Xl)]  # label is always [1, 0]\n",
    "\n",
    "            # calculate the similarity score\n",
    "            score_pos = calculate_similarity_score(compute_features, Xl, Xr_pos)\n",
    "            score_neg = calculate_similarity_score(compute_features, Xl, Xr_neg)\n",
    "            # compute the loss and accuracy\n",
    "            loss, acc = hinge_loss(score_pos, score_neg, label)\n",
    "\n",
    "            # compute the gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # let the optimizer perform one step and update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Append loss to list\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                print(\"Loss (%04d it):%.04f \\tAccuracy: %0.3f\" % (idx, loss, acc))\n",
    "    finally:\n",
    "        patches.stop()\n",
    "        print(\"Finished training!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the training! For this we provide a separate training dataset. Depending on your hardware this may take a few minutes! If you don't have access to a GPU, you can uncomment the indicated lines in the training loop helper function as well as the training code below to run the training on the CPU. Note that this will be significantly slower.\n",
    "\n",
    "## h)\n",
    "\n",
    "Try to improve the network by finding better hyperparameters. For example, you can vary the number of training iterations or the number of filters in the convolutional layers. Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility        \n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Shortcuts for directories\n",
    "model_out_dir = os.path.join(out_dir, 'model')\n",
    "model_file = os.path.join(model_out_dir, \"model.t7\")\n",
    "\n",
    "# Hyperparameters\n",
    "training_iterations = 1000\n",
    "batch_size= 128\n",
    "learning_rate = 3e-4\n",
    "patch_size = 9\n",
    "padding = patch_size // 2\n",
    "max_disparity = 50\n",
    "\n",
    "# Check if output directory exists and if not create it\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(model_out_dir):\n",
    "    os.makedirs(model_out_dir)\n",
    "\n",
    "# Create dataloader for KITTI training set\n",
    "dataset = KITTIDataset(\n",
    "    os.path.join(input_dir, \"data_scene_flow/training/\"),\n",
    "    os.path.join(input_dir, \"data_scene_flow/training/disp_noc_0\"),\n",
    ")\n",
    "# Load patch provider\n",
    "patches = PatchProvider(dataset, patch_size=(patch_size, patch_size))\n",
    "\n",
    "# Initialize the network\n",
    "compute_features = StereoMatchingNetwork()\n",
    "# Set to train\n",
    "compute_features.train()\n",
    "# uncomment if you don't have a gpu\n",
    "# compute_features.to('cpu')\n",
    "optimizer = torch.optim.SGD(compute_features.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Start training loop\n",
    "training_loop(compute_features, patches, optimizer,\n",
    "                iterations=1000, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last building block, we need a way to compute disparity maps from the network's outputs. This is implemented in the helper function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def compute_disparity_CNN(compute_features, img_l, img_r, max_disparity=50):\n",
    "    \"\"\"\n",
    "    Computes the disparity of the stereo image pair.\n",
    "\n",
    "    Args:\n",
    "        compute_features:  pytorch module object\n",
    "        img_l: tensor holding the left image\n",
    "        img_r: tensor holding the right image\n",
    "        max_disparity (int): maximum disparity\n",
    "\n",
    "    Returns:\n",
    "        D: tensor holding the disparity\n",
    "    \"\"\"\n",
    "    # get the image features by applying the similarity metric\n",
    "    Fl = compute_features(img_l[None])\n",
    "    Fr = compute_features(img_r[None])\n",
    "\n",
    "    # images of shape B x H x W x C\n",
    "    B, H, W, C = Fl.shape\n",
    "    # Initialize the disparity\n",
    "    disparity = torch.zeros((B, H, W)).int()\n",
    "    # Initialize current similarity to -infimum\n",
    "    current_similarity = torch.ones((B, H, W)) * -np.inf\n",
    "\n",
    "    # Loop over all possible disparity values\n",
    "    Fr_shifted = Fr\n",
    "    for d in range(max_disparity + 1):\n",
    "        if d > 0:\n",
    "            # initialize shifted right image\n",
    "            Fr_shifted = torch.zeros_like(Fr)\n",
    "            # insert values which are shifted to the right by d\n",
    "            Fr_shifted[:, :, d:] = Fr[:, :, :-d]\n",
    "\n",
    "        # Calculate similarities\n",
    "        sim_d = torch.sum(Fl * Fr_shifted, dim=3)\n",
    "        # Check where similarity for disparity d is better than current one\n",
    "        indices_pos = sim_d > current_similarity\n",
    "        # Enter new similarity values\n",
    "        current_similarity[indices_pos] = sim_d[indices_pos]\n",
    "        # Enter new disparity values\n",
    "        disparity[indices_pos] = d\n",
    "\n",
    "    return disparity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have fitted a model that can estimate disparity from a pair of stereo images. Let's see how well it performs on the test set by visualizing the disparity maps.\n",
    "\n",
    "## i)\n",
    "\n",
    "Compare the visualization of the disparity maps from the Siamese Neural Network to the ones obtained by the block matching algorithm. Which predictions are better and why? Can you find regions in the scenes where the differences in predictions are most dominant? (If you were not able to solve the previous exercises, you can use the provided disparity maps in the examples/ folder.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set network to eval mode\n",
    "compute_features.eval()\n",
    "compute_features.to('cpu')\n",
    "\n",
    "# Load KITTI test split\n",
    "dataset = KITTIDataset(os.path.join(input_dir, \"data_scene_flow/testing/\"))\n",
    "# Loop over test images\n",
    "for i in range(len(dataset)):\n",
    "    print('Processing %d image' % i)\n",
    "    # Load images and add padding\n",
    "    img_left, img_right = dataset[i]\n",
    "    img_left_padded, img_right_padded = add_padding(img_left, padding), add_padding(img_right, padding)\n",
    "    img_left_padded, img_right_padded = torch.Tensor(img_left_padded), torch.Tensor(img_right_padded)\n",
    "\n",
    "    disparity_map = compute_disparity_CNN(\n",
    "        compute_features, img_left_padded, img_right_padded, max_disparity=max_disparity\n",
    "    )\n",
    "    # Define title for the plot\n",
    "    title = 'Disparity map for image %04d with SNN \\\n",
    "        (training iterations %d, batch size %d, patch_size %d)' % (i, training_iterations, batch_size, patch_size)\n",
    "    visualize_disparity(disparity_map.squeeze(), img_left.squeeze(), img_right.squeeze(), title, max_disparity=max_disparity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('lecturecv-ex02': conda)",
   "name": "python388jvsc74a57bd0086cb0d6dc2a2d401ef9cebc93822b260e9817e73ec7bbf1ef35566bb3408672"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "086cb0d6dc2a2d401ef9cebc93822b260e9817e73ec7bbf1ef35566bb3408672"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
