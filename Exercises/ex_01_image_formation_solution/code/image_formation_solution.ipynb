{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh4_HRosFOG0"
   },
   "source": [
    "# Computer Vision Lecture - Exercise 1 - Image Formation\n",
    "\n",
    "In this exercise, you will gain hands-on experience regarding the image formation process and geometric transformations. More specifically, we will develop our own simple renderer and play around with focal lengths, BRDF-based shading, rotations, translations, and much more! In the second part of the notebook, we will have a look at homographies and how they can be used to stitch together images to form panoramas.\n",
    "\n",
    "This notebook guides you through the relevant steps. When you see helper functions, you don't need to do anything - they are already implemented. The functions you need to implement are indicated as Exercise Function. Sometimes, you can find Hints - these are written upside-down so you can first try to find the solution without reading them.\n",
    "\n",
    "Good luck and lot's of fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2cAIYOQISHk"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "Let's first import relevant libaries and define hyperparameters. For the latter, we set the image height H and width W to 128 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDBkWiYfiISM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from matplotlib.patches import Polygon\n",
    "import cv2\n",
    "\n",
    "# Let's first define hyperparameters. In our case, we set the image height H and width H to 128 pixels.\n",
    "H, W = 128, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_v1ONsPIiww"
   },
   "source": [
    "We need an object in our scene in order to render more than an empty image! For this, we define a helper function which returns the faces (and optionally the face normals) of a cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxtMOGgIiLwp"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def get_cube(center=(0, 0, 2), rotation_angles=[0., 0., 0.], with_normals=False, scale=1.):\n",
    "    ''' Returns an array containing the faces of a cube.\n",
    "\n",
    "    Args:\n",
    "    center (tuple): center of the cube\n",
    "    rotation_angles (tuple): Euler angles describing the rotation of the cube\n",
    "    with_normals (bool): whether to return the normal vectors of the faces\n",
    "    scale (float): scale of cube\n",
    "\n",
    "    '''\n",
    "    # A cube consists of 6 faces and 8 corners:\n",
    "    #   +----+\n",
    "    #  /    /|\n",
    "    # +----+ |\n",
    "    # |    | +\n",
    "    # |    |/\n",
    "    # +----+\n",
    "    # Let's first consider the unit cube. The corners are:\n",
    "    corners = np.array([(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)])\n",
    "    # Let's now center the cube at (0, 0, 0)\n",
    "    corners = corners - np.array([0.5, 0.5, 0.5], dtype=np.float32).reshape(1, 3)\n",
    "    # Let's scale the cube\n",
    "    corners = corners * scale\n",
    "    # And we rotate the cube wrt. the input rotation angles\n",
    "    rot_mat = R.from_euler('xyz', rotation_angles, degrees=True).as_matrix()\n",
    "    corners = np.matmul(corners, rot_mat.T)\n",
    "    # Finally, we shift the cube according to the input center tuple\n",
    "    corners = corners + np.array(center, dtype=np.float32).reshape(1, 3)\n",
    "\n",
    "    # The 6 faces of the cube are then given as:\n",
    "    faces = np.array([\n",
    "    # all faces containing (0, 0, 0)\n",
    "    [corners[0], corners[1], corners[3], corners[2]],\n",
    "    [corners[0], corners[1], corners[5], corners[4]],\n",
    "    [corners[0], corners[2], corners[6], corners[4]],\n",
    "    # all faces containing (1, 1, 1)\n",
    "    [corners[-1], corners[-2], corners[-4], corners[-3]],\n",
    "    [corners[-1], corners[-2], corners[-6], corners[-5]],\n",
    "    [corners[-1], corners[-3], corners[-7], corners[-5]],\n",
    "    ])\n",
    "\n",
    "    if with_normals:\n",
    "        normals = np.array([(-1, 0, 0), (0, -1, 0), (0, 0, -1), (1, 0, 0), (0, 1, 0), (0, 0, 1)])\n",
    "        normals = np.matmul(normals, rot_mat.T)\n",
    "        return faces, normals\n",
    "    else:\n",
    "        return faces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AmFoc5OhI5-q"
   },
   "source": [
    "## 2.1 Perspective Projection\n",
    "\n",
    "In the first part of the exercise, we will use the pinhole camera model which performs a perspective projection. After developing the projection and shading functions, we will play around with geometric transformations and analyze the effect of the focal lengths!\n",
    "\n",
    "### a)\n",
    "We arrive at our first exercise function. Your task is complete the function get_camera_intrinsics which returns a 3x3 camera matrix for provided focal lengths fx, fy and the principal point (cx, cy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ineb50epN8E"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def get_camera_intrinsics(fx=70, fy=70, cx=W/2., cy=H/2.):\n",
    "    ''' Returns the camera intrinsics matrix.\n",
    "\n",
    "    Hint: The array should be of size 3x3 and of dtype float32 (see the assertion below)\n",
    "\n",
    "    Args:\n",
    "    fx (float): focal length in x-direction f_x\n",
    "    fy (float): focal length in y-direction f_y\n",
    "    cx (float): x component of the principal point\n",
    "    cy (float): y compontent of th principal point\n",
    "    '''\n",
    "    K = np.array([\n",
    "                [fx, 0, cx],\n",
    "                [0, fy, cy],\n",
    "                [0, 0,  1],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    assert(K.shape == (3, 3) and K.dtype == np.float32)\n",
    "    return K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XHQClVxTLFmm"
   },
   "source": [
    "### b)\n",
    "\n",
    "We now develop the function `get_perspective_projection`. It takes in a 3D point in camera space `x_c` and the camera matrix `K` and it returns the point in screen space (i.e. the pixel coordinates) `x_s`. Note that we return a 2D vector for `x_s` as we drop the final value (which is always 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perspective_projection(x_c, K):\n",
    "    ''' Projects the 3D point x_c to screen space and returns the 2D pixel coordinates.\n",
    "    \n",
    "    Args:\n",
    "        x_c (array): 3D point in camera space\n",
    "        K (array): camera intrinsics matrix (3x3)\n",
    "    '''\n",
    "    assert(x_c.shape == (3,) and K.shape == (3, 3))\n",
    "    \n",
    "    # 1.) Project points3D to camera space\n",
    "    xc_projected = np.matmul(K, x_c)\n",
    "    \n",
    "    # 2.) uv is now a three-dimensional inhomogenous vector. To obtain the final pixel coordinates\n",
    "    # you need to calculate the augmented vector and then return extract the first two coordinates\n",
    "    # and return them.\n",
    "    x_s = xc_projected[:2] / xc_projected[-1]\n",
    "    assert(x_s.shape == (2,))\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide a helper function to project the entire cube using the camera matrix K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def project_cube(cube, K):\n",
    "    ''' Projects the cube.\n",
    "    \n",
    "    Args:\n",
    "        cube (array): cube\n",
    "        K (array): camera intrinsics matrix\n",
    "    '''\n",
    "    s = cube.shape\n",
    "    assert(s[-1] == 3)\n",
    "    cube = cube.reshape(-1, 3)\n",
    "    projected_cube = np.stack([get_perspective_projection(p, K) for p in cube])\n",
    "    projected_cube = projected_cube.reshape(*s[:-1], 2)\n",
    "    return projected_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP1Mr5PLNCvA"
   },
   "source": [
    "To see if our developed functions make sense, let's have a look at a projection! But first, we need to write a function which plots the projected cube - this is provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQkRJQrncZtI"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def plot_projected_cube(projected_cube, figsize=(5, 5), figtitle=None, colors=None, face_mask=None):\n",
    "    ''' Plots the projected cube.\n",
    "\n",
    "    Args:\n",
    "    projected_cube (array): projected cube (size 6x4x2)\n",
    "    figsize (tuple): size of the figure\n",
    "    colors (list): list of colors for polygons. If None, 'blue' is used for all faces\n",
    "    face_mask (array): mask for individual faces of the cube. If None, all faces are drawn.\n",
    "    '''\n",
    "    assert(projected_cube.shape == (6, 4, 2))\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if figtitle is not None:\n",
    "        fig.suptitle(figtitle)\n",
    "    if colors is None:\n",
    "        colors = ['C0' for i in range(len(projected_cube))]\n",
    "    if face_mask is None:\n",
    "        face_mask = [True for i in range(len(projected_cube))]\n",
    "    ax.set_xlim(0, W), ax.set_ylim(0, H)\n",
    "    ax.set_xlabel('Width'), ax.set_ylabel(\"Height\")\n",
    "    for (cube_face, c, mask) in zip(projected_cube, colors, face_mask):\n",
    "        if mask:\n",
    "            ax.add_patch(Polygon(cube_face, color=c))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xll7NovNVtO"
   },
   "source": [
    "Let's have a look at our first rendering of a cube! We get our camera matrix `K` and the cube `cube`, then we project it using the `project_cube` function, and finally we can visualize the projection using the `plot_projected_cube function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "5LSrqBQVcF4d",
    "outputId": "5cbb4a15-9e21-42f4-99bb-d8d61b2498b9"
   },
   "outputs": [],
   "source": [
    "K = get_camera_intrinsics()\n",
    "cube = get_cube(rotation_angles=[30, 50, 0])\n",
    "projected_cube = project_cube(cube, K)\n",
    "plot_projected_cube(projected_cube, figtitle='Projected Cube')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUSidMUgOj55"
   },
   "source": [
    "Doesn't look to bad! However, all faces have the same color, so we cannot really see which face of the cube is in front. For this, let's develop a simple shading function next!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8eyiK1ulOxIj"
   },
   "source": [
    "### c)\n",
    "\n",
    "We now develop the function `get_face_color`. For the input `normal` of a cube face and the `point_light direction` vector for a point light source, we calculate the color intensity using the rendering equation. We then stack the color intensity three times to obtain a RGB color value (scaled between 0.1 and 0.9 to avoid pure white/black). For calculating the light intensity, have a closer look at the rendering equation discussed in the lecture. You can assume that the surface does not emit light and that the BRDF term is always 1. The incoming light is exactly 1 for the direction of the point light source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def get_face_color(normal, point_light_direction=(0, 0, 1)):\n",
    "    ''' Returns the face color for input normal.\n",
    "    \n",
    "    Args:\n",
    "        normal (array): 3D normal vector\n",
    "        point_light_direction (tuple): 3D point light direction vector\n",
    "    '''\n",
    "    assert(normal.shape == (3,))\n",
    "    point_light_direction = np.array(point_light_direction, dtype=np.float32)\n",
    "    light_intensity = np.sum(normal * (-point_light_direction))\n",
    "    color_intensity = 0.1 + (light_intensity * 0.5 + 0.5) * 0.8\n",
    "    color = np.stack([color_intensity for i in range(3)])\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helper function which returns the face colors for multiple normals by looping over them and returning stacking the output of the `get_face_color` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COt1UAG8DJbu"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def get_face_colors(normals, light_direction=(0, 0, 1)):\n",
    "    ''' Returns the face colors for given normals and viewing direction.\n",
    "\n",
    "    Args:\n",
    "    normals (array): face normals (last dimension is 3)\n",
    "    light_direction (tuple): light direction vector\n",
    "    '''\n",
    "    colors = np.stack([get_face_color(normal, light_direction) for normal in normals])\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UUksJBmPhS_"
   },
   "source": [
    "When using different colors for the cube faces, we now also need to reason about which face is visible in the image. This function is provided in the following. We use a very simple heuristic here based on the face normals and the viewing direction which works for our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PSaX1NMDNAB"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def get_face_mask(cube, normals, camera_location=(0, 0, 0)):\n",
    "    ''' Returns a mask for each face of the cube whether it is visible when projected.\n",
    "    \n",
    "    Args:\n",
    "    cube (array): cube faces\n",
    "    normals (array): face normals (last dimension is 3)\n",
    "    camera_location (tuple): viewing camera location vector\n",
    "    '''\n",
    "    assert(cube.shape == (6, 4, 3) and normals.shape[-1] == 3)\n",
    "    camera_location = np.array(camera_location).reshape(1, 3) \n",
    "\n",
    "    face_center = np.mean(cube, axis=1)\n",
    "    viewing_direction = camera_location - face_center\n",
    "    dot_product = np.sum(normals * viewing_direction, axis=-1)\n",
    "    mask = dot_product > 0.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQBNNTjTP0FH"
   },
   "source": [
    "Great, let's have a look at our projected cube with shading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "Zj_mt6-KDbyL",
    "outputId": "ab494f4c-45d5-41b9-dffa-6c726f457db4"
   },
   "outputs": [],
   "source": [
    "cube, normals = get_cube(rotation_angles=[30, 50, 0], with_normals=True)\n",
    "colors = get_face_colors(normals)\n",
    "mask = get_face_mask(cube, normals)\n",
    "projected_cube = project_cube(cube, get_camera_intrinsics())\n",
    "plot_projected_cube(projected_cube, figtitle=\"Projected Cuboid with Shading\", colors=colors, face_mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z12lTdRjQAgG"
   },
   "source": [
    "Looks much better, right? We can see that **shading is a strong cue for reasoning about 3D geometry**. In the following, we will create animations where we rotate the cube or change the focal lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5OEjevLQ1qK"
   },
   "source": [
    "The next function `get_animation` returns a matplotlib animation for given lists of camera matrices and cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfOWImBuf1z4"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def get_animation(K_list, cube_list, figsize=(5, 5), title=None):\n",
    "    ''' Create a matplotlib animation for the list of camera matrices and cubes with face normals.\n",
    "\n",
    "    Args:\n",
    "    K_list (list): list of camera matrices\n",
    "    cube_list (list): list of cubes\n",
    "    figsize (tuple): matplotlib figsize\n",
    "    title (str): if not None, the title of the figure\n",
    "    '''\n",
    "    assert(len(K_list) == len(cube_list))\n",
    "\n",
    "    # split cube_list into cubes and normals\n",
    "    cubes = [i[0] for i in cube_list]\n",
    "    normals = [i[1] for i in cube_list]\n",
    "\n",
    "    # get face colors and masks\n",
    "    colors = [get_face_colors(normals_i) for normals_i in normals]\n",
    "    masks = [get_face_mask(cube_i, normals_i) for (cube_i, normals_i) in zip(cubes, normals)]\n",
    "\n",
    "    # get projected cubes\n",
    "    projected_cubes = [project_cube(cube, Ki) for (cube, Ki) in zip(cubes, K_list)]\n",
    "\n",
    "    # initialize plot\n",
    "    uv = projected_cubes[0]\n",
    "    patches = [Polygon(uv_i, closed=True, color='white') for uv_i in uv]\n",
    "\n",
    "    # Define animation function\n",
    "    def animate(n):\n",
    "        ''' Animation function for matplotlib visualizations.\n",
    "        '''\n",
    "        uv = projected_cubes[n]\n",
    "        color = colors[n]\n",
    "        mask = masks[n]\n",
    "        for patch, uv_i, color_i, mask_i in zip(patches, uv, color, mask):\n",
    "            if mask_i:\n",
    "                patch.set_xy(uv_i)\n",
    "                patch.set_color(color_i)\n",
    "            else:\n",
    "                uv_i[:] = -80\n",
    "                patch.set_color(color_i)\n",
    "                patch.set_xy(uv_i)\n",
    "        return patches\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    plt.close()\n",
    "    ax.set_xlim(0, W)\n",
    "    ax.set_ylim(0, H)\n",
    "    for patch in patches:\n",
    "        ax.add_patch(patch)\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(K_list), interval=100, blit=True)\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aILzXxiaqVh"
   },
   "source": [
    "With this helper function, we can now create cool animations! Let's start off with creating a list of camera matrices and a list of cubes with normals where we rotate the cube along the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ONYwook5hCH6",
    "outputId": "30e932a1-029b-4e40-df72-0e7f553f53a0"
   },
   "outputs": [],
   "source": [
    "K_list = [get_camera_intrinsics() for i in range(30)]\n",
    "cube_list = [get_cube(rotation_angles=[0, angle, 0], with_normals=True) for angle in np.linspace(0, 360, 30)]\n",
    "anim = get_animation(K_list, cube_list, title=\"Rotation of Cube\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8vuo3Hca65k"
   },
   "source": [
    "Next, we can analyze the effect of changing the focal lenghs. Let's start with the focal length in x-direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "y_pDazN6nF7V",
    "outputId": "a0465b2b-bbd7-42f3-9171-1e2e2837cabd"
   },
   "outputs": [],
   "source": [
    "K_list = [get_camera_intrinsics(fx=f) for f in np.linspace(10, 150, 30)]\n",
    "cube_list = [get_cube(rotation_angles=(0, 30, 50), with_normals=True) for i in range(30)]\n",
    "anim = get_animation(K_list, cube_list, title=\"Change of focal length along the x-axis.\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2gFBpMpRbOuO"
   },
   "source": [
    "### d) + e)\n",
    "\n",
    "Create a visualization where you change both focal lengths similarly to the previous example. Further, create a visualization where you translate the cube along the y-axis between the values [-2, 2].\n",
    "\n",
    "*Hint 1*: ˙uoᴉʇɔunɟ ǝqnɔ‾ʇǝƃ ǝɥʇ ɟo ʇuǝɯnƃɹɐ ɹǝʇuǝɔ ǝɥʇ ǝʇɐlndᴉuɐɯ 'ǝqnɔ ǝɥʇ ƃuᴉʇɐlsuɐɹʇ ɹoℲ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "so1640-uhRFi",
    "outputId": "7739d469-f35f-407b-983d-18a112429c4a"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "K_list = [get_camera_intrinsics(fx=f, fy=f) for f in np.linspace(10, 150, 30)]\n",
    "cube_list = [get_cube(rotation_angles=(0, 30, 50), with_normals=True) for i in range(30)]\n",
    "anim = get_animation(K_list, cube_list, title=\"Change of focal length along both axes.\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "mpLB6NkAbc1G",
    "outputId": "36a24417-bb1e-4ed8-8911-3a13b4363335"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "K_list = [get_camera_intrinsics() for i in range(30)]\n",
    "cube_list = [get_cube(center=(i, 0, 2), rotation_angles=(0, 30, 50), with_normals=True) for i in np.linspace(-2, 2, 30)]\n",
    "anim = get_animation(K_list, cube_list, title=\"Change of cube translation along y-axis.\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BX_SchwogJZl"
   },
   "source": [
    "### f) - Dolly Zoom Effect\n",
    "\n",
    "Great! As you are now familiar with manipulating the focal lengths as well as translating the object, we can now even create the Dolly Zoom effect! For this, linearly change the focal lengths between 10 and 150 while also translating the cube along the z-axis between 0.9 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "HsdgrP7Obq5I",
    "outputId": "bc954c75-afa0-403a-c6e8-74f253eacc8b"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "K_list = [get_camera_intrinsics(fx=f, fy=f) for f in np.linspace(30, 500, 30)]\n",
    "cube_list = [get_cube(center=(0, 0, i), rotation_angles=(30, 50, 0), with_normals=True) for i in np.linspace(1., 10, 30)]\n",
    "anim = get_animation(K_list, cube_list, title=\"Dolly Zoom Effect.\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU0snL6JhMV9"
   },
   "source": [
    "Great! If you implemented it correctly, the size of the cube should not change while the perspective distortion changes (this effect has also been used in movies, check out https://en.wikipedia.org/wiki/Dolly_zoom for more information)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rGB30pLsjkD4"
   },
   "source": [
    "## 2.2 Comparison of Perspective and Orthographic Projection\n",
    "\n",
    "We have now seen the effect of a perspective transformation. Let's contrast this against an orthographic projection in the following!\n",
    "\n",
    "### a)\n",
    "\n",
    "Complete the function `get_orthographic_projection` which maps an input point in camera space `x_c` to 2D pixel coordinates `x_s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Af6HqlExj_V4"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def get_orthographic_projection(x_c):\n",
    "    ''' Projects the 3D point in camera space x_c to 2D pixel coordinates using an orthographic projection.\n",
    "    \n",
    "    Args:\n",
    "        x_c (array): 3D point in camera space\n",
    "    '''\n",
    "    assert(x_c.shape == (3,))\n",
    "\n",
    "    # 1.) Define 2x3 projection matrix\n",
    "    K = np.array([\n",
    "                [1, 0, 0],\n",
    "                [0, 1, 0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # 1.) Project points3D to camera space\n",
    "    x_s = np.matmul(K, x_c)\n",
    "    assert(x_s.shape == (2,))\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide a helper function to project the entire cube using the `get_orthographic_projection` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_cube_orthographic(cube):\n",
    "    ''' Projects the cube using an orthographic projection.\n",
    "    \n",
    "    Args:\n",
    "        cube (array): cube\n",
    "    '''\n",
    "    s = cube.shape\n",
    "    assert(s[-1] == 3)\n",
    "    cube = cube.reshape(-1, 3)\n",
    "    projected_cube = np.stack([get_orthographic_projection(p) for p in cube])\n",
    "    projected_cube = projected_cube.reshape(*s[:-1], 2)\n",
    "    return projected_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9Aekr59k4yg"
   },
   "source": [
    "Now, let's have a look how the cube from earlier looks like when we project it via the orthographic projection. (To obtain a similar-sized cube, we need to increase its scale to account for the missing scale factor; instead, we could also use a scaled orthographic projection, but we choose this approach for simplicity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "HwQtgvfuk_NN",
    "outputId": "3a574466-0740-434d-b3a4-c42b87f559a7"
   },
   "outputs": [],
   "source": [
    "cube, normals = get_cube(center=(60., 60., 100), rotation_angles=[30, 50, 0], scale=60., with_normals=True)\n",
    "colors = get_face_colors(normals)\n",
    "mask = get_face_mask(cube, normals)\n",
    "projected_cube = project_cube_orthographic(cube)\n",
    "plot_projected_cube(projected_cube, figtitle=\"Orthographic-Projected Cube with Shading\", colors=colors, face_mask=mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v9kBWVs9pu7y"
   },
   "source": [
    "### b)\n",
    "\n",
    "How would you describe your result? Compare it against the visualization of the dolly zoom. When does the perspective projection look most similar to the orthographic projection?\n",
    "\n",
    "To confirm your analysis, plot a cube with center `(0, 0, 150)` and the same rotation angles `(30, 50, 0)` which you project with a camera matrix with focal lengths of `10000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "IUrQnLrMrsMp",
    "outputId": "3aee749f-2cdf-47d9-927b-559dbeaf0516"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "cube, normals = get_cube(center=(0, 0, 150), rotation_angles=[30, 50, 0], with_normals=True)\n",
    "K = get_camera_intrinsics(fx=10000, fy=10000)\n",
    "colors = get_face_colors(normals)\n",
    "mask = get_face_mask(cube, normals)\n",
    "projected_cube = project_cube(cube, K)\n",
    "plot_projected_cube(projected_cube, figtitle=\"Projected Cube with shading and large distance and large focal lengths\", colors=colors, face_mask=mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Panorama Stitching and DLT\n",
    "\n",
    "In the second part of the exercise, we have a look at homographies and how they can be used to stitch together photos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the two images we want to stich together and the pre-computed point pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = cv2.cvtColor(cv2.imread('./image-1.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('./image-2.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load matching points\n",
    "npz_file = np.load('./panorama_points.npz')\n",
    "points_source = npz_file['points_source']\n",
    "points_target = npz_file['points_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the images\n",
    "f = plt.figure(figsize=(15, 5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.imshow(img1)\n",
    "ax2.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also have a look at some correspondence pairs! For this, the `draw_matches` function is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def draw_matches(img1, points_source, img2, points_target):\n",
    "    ''' Returns an image with matches drawn onto the images.\n",
    "    '''\n",
    "    r, c = img1.shape[:2]\n",
    "    r1, c1 = img2.shape[:2]\n",
    "\n",
    "    output_img = np.zeros((max([r, r1]), c + c1, 3), dtype='uint8')\n",
    "    output_img[:r, :c, :] = np.dstack([img1])\n",
    "    output_img[:r1, c:c + c1, :] = np.dstack([img2])\n",
    "\n",
    "    for p1, p2 in zip(points_source, points_target):\n",
    "        (x1, y1) = p1[:2]\n",
    "        (x2, y2) = p2[:2]\n",
    "\n",
    "        cv2.circle(output_img, (int(x1), int(y1)), 10, (0, 255, 255), 10)\n",
    "        cv2.circle(output_img, (int(x2) + c, int(y2)), 10, (0, 255, 255), 10)\n",
    "\n",
    "        cv2.line(output_img, (int(x1), int(y1)), (int(x2) + c, int(y2)), (0, 255, 255), 5)\n",
    "\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20, 10))\n",
    "vis = draw_matches(img1, points_source[:5], img2, points_target[:5])\n",
    "plt.imshow(vis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "After looking at the correspondences, let's stitch the images together! In order to stich together the images, we need a function to return the 2x9 \"A_i\" matrix discussed in the lecture for a given 2D correspondence pair `xi_vector` and `xi_prime_vector` (these are 3D homogeneous vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def get_Ai(xi_vector, xi_prime_vector):\n",
    "    ''' Returns the A_i matrix discussed in the lecture for input vectors.\n",
    "    \n",
    "    Args:\n",
    "        xi_vector (array): the x_i vector in homogeneous coordinates\n",
    "        xi_vector_prime (array): the x_i_prime vector in homogeneous coordinates\n",
    "    '''\n",
    "    assert(xi_vector.shape == (3,) and xi_prime_vector.shape == (3,))\n",
    "    zero_vector = np.zeros((3,), dtype=np.float32)\n",
    "    xi, yi, wi = xi_prime_vector\n",
    "    \n",
    "    Ai = np.stack([\n",
    "        np.concatenate([zero_vector, -wi*xi_vector, yi*xi_vector]),\n",
    "        np.concatenate([wi*xi_vector, zero_vector, -xi*xi_vector]),\n",
    "        # np.concatenate([-yi*xi_vector, xi*xi_vector, zero_vector]) this is not needed, so we comment it out\n",
    "    ])\n",
    "    assert(Ai.shape == (2, 9))\n",
    "    return Ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Using `get_Ai`, write a function `get_A` which returns the A matrix of size 2Nx9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def get_A(points_source, points_target):\n",
    "    ''' Returns the A matrix discussed in the lecture.\n",
    "    \n",
    "    Args:\n",
    "        points_source (array): 3D homogeneous points from source image\n",
    "        points_target (array): 3D homogeneous points from target image\n",
    "    '''\n",
    "    N = points_source.shape[0]\n",
    "    correspondence_pairs = zip(points_source, points_target)\n",
    "    \n",
    "    A = np.concatenate([get_Ai(p1, p2) for (p1, p2) in correspondence_pairs])\n",
    "    assert(A.shape == (2*N, 9))\n",
    "    return A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Next, implement the function `get_homography` which returns the homography H for point corrspondence pairs. You should obtain H by performing the Direct Linear Transformation (DLT) algorithm (consisting of 3 steps).\n",
    "\n",
    "*Hint 1*: ˙uoᴉʇɔunɟ ∀ʇǝƃ ǝɥʇ ƃuᴉsn xᴉɹʇɐɯ ∀ ǝɥʇ uᴉɐʇqo ʇsɹᴉɟ\n",
    "\n",
    "*Hint 2*: ˙(pʌs˙ƃlɐuᴉl˙du ƃuᴉsn ƃǝ) ∀ uo pΛS ɯɹoɟɹǝd 'ʇxǝu\n",
    "\n",
    "*Hint 3*: ˙xᴉɹʇɐɯ Λ pǝsodsuɐɹʇ ǝɥʇ ɟo ǝnlɐʌ ʇsɐl ǝɥʇ ʇɔǝlǝs 'ʎllɐuᴉɟ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "def get_homography(points_source, points_target):\n",
    "    ''' Returns the homography H.\n",
    "    \n",
    "    Args:\n",
    "        points_source (array): 3D homogeneous points from source image\n",
    "        points_target (array): 3D homogeneous points from target image        \n",
    "    '''\n",
    "    A = get_A(points_source, points_target)\n",
    "    u, s, vh = np.linalg.svd(A)\n",
    "    H = vh[-1].reshape(3, 3)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function which takes in the two images and the calculated homography and it returns the stiched image in a format which we can display easy with matplotlib. This function is provided in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def stich_images(img1, img2, H):\n",
    "    ''' Stitches together the images via given homography H.\n",
    "\n",
    "    Args:\n",
    "        img1 (array): image 1\n",
    "        img2 (array): image 2\n",
    "        H (array): homography\n",
    "    '''\n",
    "\n",
    "    rows1, cols1 = img1.shape[:2]\n",
    "    rows2, cols2 = img2.shape[:2]\n",
    "\n",
    "    list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)\n",
    "    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n",
    "\n",
    "    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n",
    "    list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n",
    "\n",
    "    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n",
    "    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "    translation_dist = [-x_min,-y_min]\n",
    "\n",
    "    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n",
    "\n",
    "    output_img = cv2.warpPerspective(img2, H_translation.dot(H), (x_max-x_min, y_max-y_min))\n",
    "    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n",
    "\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can have a look at our panorama! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = get_homography(points_target, points_source)\n",
    "stiched_image = stich_images(img1, img2, H)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "fig.suptitle(\"Stiched Panorama\")\n",
    "plt.imshow(stiched_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Now, it's your turn: Create your own panorama! Go out and take two photos with your smartphone or camera and save them to your computer. Make sure that you only change the angle of the phone/camera, not the position! Save them to the lecture exercise folder and cange the file paths for `img1` and `img2` below to your own images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#### Exercise Function ####\n",
    "###########################\n",
    "# Load images\n",
    "img1 = cv2.cvtColor(cv2.imread('./image-1.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('./image-2.jpg'), cv2.COLOR_BGR2RGB)\n",
    "# Let's visualize the images\n",
    "f = plt.figure(figsize=(15, 5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.imshow(img1)\n",
    "ax2.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For obtaining the homography, we need correspondence pairs. In the following we provide a function for this based on feature matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### Helper Function #####\n",
    "###########################\n",
    "def get_keypoints(img1, img2):\n",
    "    orb = cv2.ORB_create(nfeatures=2000)\n",
    "\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n",
    "\n",
    "    # Find matching points\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good.append(m)\n",
    "    p_source = np.float32([ keypoints1[good_match.queryIdx].pt for good_match in good ]).reshape(-1,2)\n",
    "    p_target = np.float32([ keypoints2[good_match.trainIdx].pt for good_match in good ]).reshape(-1,2)\n",
    "    N = p_source.shape[0]\n",
    "    p_source = np.concatenate([p_source, np.ones((N, 1))], axis=-1)\n",
    "    p_target = np.concatenate([p_target, np.ones((N, 1))], axis=-1)\n",
    "    return p_source, p_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at your own panorama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_source, p_target = get_keypoints(img1, img2)\n",
    "H = get_homography(p_target, p_source)\n",
    "stiched_image = stich_images(img1, img2, H)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "fig.suptitle(\"Stiched Panorama\")\n",
    "plt.imshow(stiched_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You now gained hands-on experience in the fields of image formation and panorama stitching!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lecture_cv_projection2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
